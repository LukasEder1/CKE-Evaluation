{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26c19178",
   "metadata": {},
   "source": [
    "# Keyword Collection Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97a52cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukas/ml/lib/python3.6/site-packages/requests/__init__.py:104: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "from contrastive_keyword_extraction import contrastive_extraction, final_score\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from policy_processing import *\n",
    "from cleantext import clean\n",
    "from baselines import *\n",
    "from tqdm import trange\n",
    "import string\n",
    "import pickle\n",
    "import sentence_comparision\n",
    "import sentence_importance\n",
    "import summary\n",
    "import utilities\n",
    "import news_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f93e9af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('/home/lukas/Documents/semester6/NLP/newsEdits/datasets/small10k.sqlite')\n",
    "conn_news = sqlite3.connect('/home/lukas/Documents/semester6/NLP/newsEdits/datasets/ap-matched-sentences.db')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "885a2b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning_func = lambda text : clean(text,\n",
    "    fix_unicode=True,               # fix various unicode errors\n",
    "    to_ascii=True,                  # transliterate to closest ASCII representation\n",
    "    lower=False,                     # lowercase text\n",
    "    no_line_breaks=False,           # fully strip line breaks as opposed to only normalizing them\n",
    "    no_urls=False,                  # replace all URLs with a special token\n",
    "    no_emails=False,                # replace all email addresses with a special token\n",
    "    no_phone_numbers=False,         # replace all phone numbers with a special token\n",
    "    no_numbers=False,               # replace all numbers with a special token\n",
    "    no_digits=False,                # replace all digits with a special token\n",
    "    no_currency_symbols=False,      # replace all currency symbols with a special token\n",
    "    no_punct=False,                 # remove punctuations\n",
    "    replace_with_punct=\"\",          # instead of removing punctuations you may replace them\n",
    "    replace_with_url=\"<URL>\",\n",
    "    replace_with_email=\"<EMAIL>\",\n",
    "    replace_with_phone_number=\"<PHONE>\",\n",
    "    replace_with_number=\"<NUMBER>\",\n",
    "    replace_with_digit=\"0\",\n",
    "    replace_with_currency_symbol=\"<CUR>\",\n",
    "    lang=\"en\"              \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f196a451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_collection(conn, \n",
    "                      sites, \n",
    "                      ke_extractor = keyword_extraction.extract_yake, \n",
    "                      num_keywords=10,\n",
    "                      max_ngram=2, \n",
    "                      sentence_matcher = sentence_comparision.match_sentences_semantic_search,\n",
    "                      importance_estimator = sentence_importance.text_rank_importance,\n",
    "                      use_furthest=False, \n",
    "                      name_prefix=\"\",\n",
    "                      make_data_persistent=False, \n",
    "                      path=\"dataframes\",\n",
    "                      threshold=0.6,\n",
    "                      stopwords=[],\n",
    "                      combinator=utilities.alpha_combination,\n",
    "                      gamma = 0.5,\n",
    "                      num_splits=1,\n",
    "                      is_policy=True,\n",
    "                      matching_model=\"all-MiniLM-L6-v2\"):\n",
    "    \n",
    "    for i in trange(len(sites)):\n",
    "\n",
    "        site_id = sites[i]\n",
    "        \n",
    "        if is_policy:\n",
    "            # sort first by year, then by phase\n",
    "            df = pd.read_sql(\"SELECT * FROM small10k\", con=conn)\n",
    "            \n",
    "            data = create_data(df.sort_values(by=['year', 'phase']), site_id)\n",
    "\n",
    "            # get_the actual strings\n",
    "            policy_texts = get_policy_texts(data)\n",
    "\n",
    "            # cleaned documents using above function\n",
    "            documents = clean_text(policy_texts, cleaning_func)\n",
    "        else:\n",
    "            documents = news_processing.parse_html_to_string(article_id, conn)\n",
    "\n",
    "        documents = [documents[0], documents[-1]]\n",
    "        \n",
    "        # run CKE-pipeline\n",
    "        # Extract Keywords, and Matched sentences\n",
    "        keywords, matched_dict, changed_indices, additions, deletions, new_indices, ranking, removed, matched_indices, unified_delitions = contrastive_extraction(documents, \n",
    "                                                                            max_ngram=max_ngram,\n",
    "                                                                            min_ngram=1, \n",
    "                                                                            show_changes=False, \n",
    "                                                                            symbols_to_remove=string.punctuation,\n",
    "                                                                            importance_estimator=importance_estimator,\n",
    "                                                                            match_sentences=sentence_matcher,\n",
    "                                                                            threshold=threshold,\n",
    "                                                                            extra_stopwords=stopwords,\n",
    "                                                                            top_k=int(num_splits),\n",
    "                                                                            combinator=combinator,\n",
    "                                                                            alpha_gamma=gamma,\n",
    "                                                                            matching_model=matching_model)\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "        # create itermediate\n",
    "        kws, scores = extract_from_dict(keywords)\n",
    "        \n",
    "        pipeline_frame = pd.DataFrame({'keyword': kws, 'score': scores})\n",
    "        \n",
    "        #extractor = lambda x: ke_extractor(x, max_ngram_size=max_ngram, numOfKeywords=num_keywords)\n",
    "        \n",
    "        # create CKE on the specified baseline\n",
    "        baseline_keywords = baseline_diff_content(additions, unified_delitions, ke_extractor, num_keywords, max_ngram)\n",
    "        \n",
    "        baseline_kws, baseline_scores = extract_from_tuple_list(baseline_keywords)\n",
    "        \n",
    "        baseline_frame1 = pd.DataFrame({'keyword': baseline_kws, 'score': baseline_scores})\n",
    "        \n",
    "        \n",
    "        # create CKE for baseline method 2\n",
    "        baseline_keywords2 = baseline_keywords_in_diff(documents, ke_extractor, additions, deletions, candidates=50, max_ngram=max_ngram)\n",
    "        \n",
    "        baseline_kws2, baseline_scores2 = extract_from_dict(baseline_keywords2)\n",
    "        \n",
    "        baseline_frame2 = pd.DataFrame({'keyword': baseline_kws2, 'score': baseline_scores2})\n",
    "        \n",
    "        # Baseline 3\n",
    "        baseline_keywords3 = baseline3(documents, additions, unified_delitions, max_ngram)\n",
    "        \n",
    "        baseline_kws3, baseline_scores3 = extract_from_dict(baseline_keywords3)\n",
    "        \n",
    "        baseline_frame3 = pd.DataFrame({'keyword': baseline_kws3, 'score': baseline_scores3})\n",
    "\n",
    "        # Baseline 4\n",
    "        baseline_keywords4 = baseline4(documents, max_ngram, stopwords)\n",
    "        \n",
    "        baseline_kws4, baseline_scores4 = extract_from_dict(baseline_keywords4)\n",
    "        \n",
    "        baseline_frame4 = pd.DataFrame({'keyword': baseline_kws4, 'score': baseline_scores4})\n",
    "\n",
    "        # decide, wether to actually save the data\n",
    "        if make_data_persistent:\n",
    "            \n",
    "            pipeline_frame.to_csv(f\"{path}/{name_prefix}_pipeline_keywords_{site_id}.csv\", index=False)\n",
    "            \n",
    "            baseline_frame1.to_csv(f\"{path}/{name_prefix}_baseline1_keywords_{site_id}.csv\", index=False)\n",
    "            \n",
    "            baseline_frame2.to_csv(f\"{path}/{name_prefix}_baseline2_keywords_{site_id}.csv\", index=False)\n",
    "            \n",
    "            baseline_frame3.to_csv(f\"{path}/{name_prefix}_baseline3_keywords_{site_id}.csv\", index=False)\n",
    "            \n",
    "            baseline_frame4.to_csv(f\"{path}/{name_prefix}_baseline4_keywords_{site_id}.csv\", index=False)\n",
    "    \n",
    "    return pipeline_frame, baseline_frame1, baseline_frame2, baseline_frame3, baseline_frame4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b178d6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [90536,\n",
    " 90344,\n",
    " 98640,\n",
    " 98585,\n",
    " 99880,\n",
    " 108079,\n",
    " 90555,\n",
    " 90545,\n",
    " 98553,\n",
    " 98572,\n",
    " 98659,\n",
    " 98706,\n",
    " 108052,\n",
    " 108097,\n",
    " 100541,\n",
    " 108771,\n",
    " 2435,\n",
    " 100595,\n",
    " 108778,\n",
    " 100419,\n",
    " 108438,\n",
    " 108835,\n",
    " 106348,\n",
    " 106486,\n",
    " 90041]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "042df25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [11:17<00:00, 27.11s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>app</td>\n",
       "      <td>0.132400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mobile</td>\n",
       "      <td>0.057117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mobile app</td>\n",
       "      <td>0.057117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>may</td>\n",
       "      <td>0.037800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>certain</td>\n",
       "      <td>0.037578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>additional</td>\n",
       "      <td>0.031150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>choice</td>\n",
       "      <td>0.031150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>within</td>\n",
       "      <td>0.031150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>listed</td>\n",
       "      <td>0.028183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>use</td>\n",
       "      <td>0.022545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>available</td>\n",
       "      <td>0.021473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>provide</td>\n",
       "      <td>0.020766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>pages</td>\n",
       "      <td>0.020668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>services</td>\n",
       "      <td>0.020131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>collection</td>\n",
       "      <td>0.019539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>via</td>\n",
       "      <td>0.018789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>certain services</td>\n",
       "      <td>0.018789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>services pages</td>\n",
       "      <td>0.018789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>pages listed</td>\n",
       "      <td>0.018789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>listed may</td>\n",
       "      <td>0.018789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>may available</td>\n",
       "      <td>0.018789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>available via</td>\n",
       "      <td>0.018789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>via mobile</td>\n",
       "      <td>0.018789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>notice</td>\n",
       "      <td>0.015575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>options</td>\n",
       "      <td>0.015575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>may provide</td>\n",
       "      <td>0.015575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>provide additional</td>\n",
       "      <td>0.015575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>additional notice</td>\n",
       "      <td>0.015575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>notice choice</td>\n",
       "      <td>0.015575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>choice options</td>\n",
       "      <td>0.015575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>options within</td>\n",
       "      <td>0.015575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>within app</td>\n",
       "      <td>0.015575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>privacy policy</td>\n",
       "      <td>0.014654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>privacy</td>\n",
       "      <td>0.013026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>data</td>\n",
       "      <td>0.011723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>policy</td>\n",
       "      <td>0.010855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>governs</td>\n",
       "      <td>0.009769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>prn</td>\n",
       "      <td>0.009769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>policy governs</td>\n",
       "      <td>0.009769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>governs collection</td>\n",
       "      <td>0.009769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>collection use</td>\n",
       "      <td>0.009769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>use data</td>\n",
       "      <td>0.009769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>data use</td>\n",
       "      <td>0.009769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>use prn</td>\n",
       "      <td>0.009769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>prn mobile</td>\n",
       "      <td>0.009769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               keyword     score\n",
       "0                  app  0.132400\n",
       "1               mobile  0.057117\n",
       "2           mobile app  0.057117\n",
       "3                  may  0.037800\n",
       "4              certain  0.037578\n",
       "5           additional  0.031150\n",
       "6               choice  0.031150\n",
       "7               within  0.031150\n",
       "8               listed  0.028183\n",
       "9                  use  0.022545\n",
       "10           available  0.021473\n",
       "11             provide  0.020766\n",
       "12               pages  0.020668\n",
       "13            services  0.020131\n",
       "14          collection  0.019539\n",
       "15                 via  0.018789\n",
       "16    certain services  0.018789\n",
       "17      services pages  0.018789\n",
       "18        pages listed  0.018789\n",
       "19          listed may  0.018789\n",
       "20       may available  0.018789\n",
       "21       available via  0.018789\n",
       "22          via mobile  0.018789\n",
       "23              notice  0.015575\n",
       "24             options  0.015575\n",
       "25         may provide  0.015575\n",
       "26  provide additional  0.015575\n",
       "27   additional notice  0.015575\n",
       "28       notice choice  0.015575\n",
       "29      choice options  0.015575\n",
       "30      options within  0.015575\n",
       "31          within app  0.015575\n",
       "32      privacy policy  0.014654\n",
       "33             privacy  0.013026\n",
       "34                data  0.011723\n",
       "35              policy  0.010855\n",
       "36             governs  0.009769\n",
       "37                 prn  0.009769\n",
       "38      policy governs  0.009769\n",
       "39  governs collection  0.009769\n",
       "40      collection use  0.009769\n",
       "41            use data  0.009769\n",
       "42            data use  0.009769\n",
       "43             use prn  0.009769\n",
       "44          prn mobile  0.009769"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_collection(conn, \n",
    "                  ids, \n",
    "                  ke_extractor = keyword_extraction.extract_yake, \n",
    "                  num_keywords=15,\n",
    "                  max_ngram=2, \n",
    "                  sentence_matcher = sentence_comparision.match_sentences_semantic_search,\n",
    "                  importance_estimator = sentence_importance.text_rank_importance,\n",
    "                  use_furthest=False, \n",
    "                  name_prefix=\"standard\",\n",
    "                  make_data_persistent=True, \n",
    "                  path=\"dataframes\",\n",
    "                  threshold=0.65,\n",
    "                  stopwords=nltk.corpus.stopwords.words(\"english\"),\n",
    "                  combinator=utilities.alpha_combination,\n",
    "                  gamma = 0.5,\n",
    "                  num_splits=1,\n",
    "                  is_policy=True,\n",
    "                  matching_model =\"msmarco-distilbert-base-v4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3261d295",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "\n",
    "def cartesian_product(params):\n",
    "    \n",
    "    # gett all possible combinations\n",
    "    return list(product(*params.values()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a582c3af",
   "metadata": {},
   "source": [
    "# Parameters to Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b825a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"matcher\": [sentence_comparision.match_sentences_semantic_search,\n",
    "                         sentence_comparision.match_sentences_tfidf_weighted],\n",
    "              \n",
    "             \"ie\": [sentence_importance.text_rank_importance,\n",
    "                    sentence_importance.yake_weighted_importance],\n",
    "              \n",
    "             \"threshold\": [0.5, 0.6, 0.7]\n",
    "              \n",
    "             }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9ef939c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cartesian_product(parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0477778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cartesian_collection(params, \n",
    "                                df,\n",
    "                                sites, \n",
    "                                baseline_ke_extractor = keyword_extraction.extract_yake, \n",
    "                                num_keywords=10, \n",
    "                                max_ngram=2, \n",
    "                                use_furthest=False,\n",
    "                                make_data_persistent=False,\n",
    "                                file_prefix = \"combination\",\n",
    "                                path=\"dataframes\",\n",
    "                                compare_k = 15):\n",
    "            \n",
    "        \n",
    "    combinations = cartesian_product(params)\n",
    "    \n",
    "    number_of_combinations = len(combinations)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for combination in combinations:\n",
    "        \n",
    "        matcher, threshold = combination\n",
    "        \n",
    "        print(f\"Contrastive Keyword Extraction pipeline is being ran with combination {count}:\")\n",
    "        \n",
    "        total_frame, intermediate_frame, baseline_frame, baseline_frame2 = create_collection(df = df, \n",
    "                                                                  sites = sites, \n",
    "                                                                  ke_extractor = baseline_ke_extractor, \n",
    "                                                                  num_keywords = num_keywords, \n",
    "                                                                  max_ngram = max_ngram, \n",
    "                                                                  sentence_matcher = matcher,\n",
    "                                                                  use_furthest = use_furthest,\n",
    "                                                                  name_prefix=f\"{file_prefix}_{count}\",\n",
    "                                                                  make_data_persistent=make_data_persistent,\n",
    "                                                                  path=path,\n",
    "                                                                  threshold=threshold)\n",
    "        \n",
    "        \n",
    "        baselines = [\"baseline1\", \"baseline2\"]\n",
    "        for baseline in baselines:\n",
    "            summary.extensive_summary(sites, \n",
    "                                      show_results=False, \n",
    "                                      k=compare_k,\n",
    "                                      name_a = f\"{file_prefix}_{count}_inter_keywords\", \n",
    "                                      name_b = f\"{file_prefix}_{count}_{baseline}_keywords\",\n",
    "                                      save_prefix=f\"{file_prefix}_{count}_{baseline}_\", \n",
    "                                      path=path)\n",
    "        \n",
    "        \n",
    "        count += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913718fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "med_ids =  [106750, 108123, 90555, 98640, 108026, 90344, 98553, 98659, 108079, 90536, 98572, 98706, 108120, 90545, 98585, 99880] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5265c57b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575da15d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316c4144",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_cartesian_collection(parameters,\n",
    "                            df,\n",
    "                            small+med_ids,\n",
    "                            baseline_ke_extractor = keyword_extraction.extract_yake,\n",
    "                            num_keywords=20,\n",
    "                            max_ngram=2,\n",
    "                            use_furthest=True, # only compare the first and last document\n",
    "                            make_data_persistent=True,\n",
    "                            file_prefix=\"threshold\",\n",
    "                            path=\"Combinations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

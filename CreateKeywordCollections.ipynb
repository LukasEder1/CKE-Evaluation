{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LukasEder1/CKE-Evaluation/blob/master/CreateKeywordCollections.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone \"https://github.com/LukasEder1/CKE-Evaluation.git\"\n",
        "%cd \"/content/CKE-Evaluation\""
      ],
      "metadata": {
        "id": "3kjdOh3ZhOGL",
        "outputId": "1c1b38f2-bccb-466c-ecaf-504c46aeb93b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "3kjdOh3ZhOGL",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CKE-Evaluation'...\n",
            "remote: Enumerating objects: 34, done.\u001b[K\n",
            "remote: Counting objects: 100% (34/34), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 34 (delta 5), reused 15 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (34/34), 170.52 KiB | 2.13 MiB/s, done.\n",
            "/content/CKE-Evaluation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "pCIq0vAtihP6",
        "outputId": "0b6115e8-63c1-4158-aa11-1b65a33257ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "pCIq0vAtihP6",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/boudinfl/pke.git (from -r requirements.txt (line 15))\n",
            "  Cloning https://github.com/boudinfl/pke.git to /tmp/pip-req-build-zsaulavc\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/boudinfl/pke.git /tmp/pip-req-build-zsaulavc\n",
            "  Resolved https://github.com/boudinfl/pke.git to commit 8f1d05dcc52041c9920ba0f9d5231fe6086d12c4\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sumy\n",
            "  Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 KB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 2)) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 3)) (1.10.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 4)) (3.4.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 5)) (1.2.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 6)) (3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 7)) (1.3.5)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.19.0-py2.py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 KB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cdifflib\n",
            "  Downloading cdifflib-1.2.6.tar.gz (11 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 11)) (1.13.1+cu116)\n",
            "Collecting yake\n",
            "  Downloading yake-0.4.8-py2.py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.2/60.2 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting summa\n",
            "  Downloading summa-1.2.0.tar.gz (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 KB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting keybert\n",
            "  Downloading keybert-0.7.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting st-annotated-text\n",
            "  Downloading st-annotated-text-3.0.0.tar.gz (7.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pysbd\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting breadability>=0.1.20\n",
            "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pycountry>=18.2.23\n",
            "  Downloading pycountry-22.3.5.tar.gz (10.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.8/dist-packages (from sumy->-r requirements.txt (line 1)) (2.25.1)\n",
            "Collecting docopt<0.7,>=0.6.1\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy->-r requirements.txt (line 4)) (1.0.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy->-r requirements.txt (line 4)) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy->-r requirements.txt (line 4)) (8.1.7)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy->-r requirements.txt (line 4)) (0.10.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy->-r requirements.txt (line 4)) (0.10.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy->-r requirements.txt (line 4)) (3.0.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy->-r requirements.txt (line 4)) (2.0.7)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy->-r requirements.txt (line 4)) (4.64.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy->-r requirements.txt (line 4)) (3.1.2)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy->-r requirements.txt (line 4)) (0.7.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy->-r requirements.txt (line 4)) (2.0.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy->-r requirements.txt (line 4)) (1.10.5)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy->-r requirements.txt (line 4)) (3.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy->-r requirements.txt (line 4)) (23.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy->-r requirements.txt (line 4)) (6.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy->-r requirements.txt (line 4)) (57.4.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy->-r requirements.txt (line 4)) (2.4.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy->-r requirements.txt (line 4)) (1.0.9)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->-r requirements.txt (line 5)) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->-r requirements.txt (line 6)) (8.1.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk->-r requirements.txt (line 6)) (2022.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->-r requirements.txt (line 7)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->-r requirements.txt (line 7)) (2022.7.1)\n",
            "Requirement already satisfied: protobuf<4,>=3.12 in /usr/local/lib/python3.8/dist-packages (from streamlit->-r requirements.txt (line 8)) (3.19.6)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.8/dist-packages (from streamlit->-r requirements.txt (line 8)) (4.5.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.8/dist-packages (from streamlit->-r requirements.txt (line 8)) (0.10.2)\n",
            "Collecting semver\n",
            "  Downloading semver-2.13.0-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.8/dist-packages (from streamlit->-r requirements.txt (line 8)) (6.2)\n",
            "Collecting validators>=0.2\n",
            "  Downloading validators-0.20.0.tar.gz (30 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting rich>=10.11.0\n",
            "  Downloading rich-13.3.2-py3-none-any.whl (238 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.7/238.7 KB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tzlocal>=1.1 in /usr/local/lib/python3.8/dist-packages (from streamlit->-r requirements.txt (line 8)) (1.5.1)\n",
            "Collecting pympler>=0.9\n",
            "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 KB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from streamlit->-r requirements.txt (line 8)) (4.2.2)\n",
            "Requirement already satisfied: pyarrow>=4.0 in /usr/local/lib/python3.8/dist-packages (from streamlit->-r requirements.txt (line 8)) (9.0.0)\n",
            "Collecting watchdog\n",
            "  Downloading watchdog-2.3.1-py3-none-manylinux2014_x86_64.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.6/80.6 KB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitpython!=3.1.19\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck>=0.1.dev5\n",
            "  Downloading pydeck-0.8.0-py2.py3-none-any.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting blinker>=1.0.0\n",
            "  Downloading blinker-1.5-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.8/dist-packages (from streamlit->-r requirements.txt (line 8)) (6.0.0)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.8/dist-packages (from streamlit->-r requirements.txt (line 8)) (5.3.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from streamlit->-r requirements.txt (line 8)) (8.4.0)\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence-transformers->-r requirements.txt (line 9)) (0.14.1+cu116)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0\n",
            "  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from yake->-r requirements.txt (line 12)) (3.0)\n",
            "Collecting segtok\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Collecting jellyfish\n",
            "  Downloading jellyfish-0.9.0.tar.gz (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 KB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from yake->-r requirements.txt (line 12)) (0.8.10)\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 KB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from pke==2.0.0->-r requirements.txt (line 15)) (0.16.0)\n",
            "Collecting htbuilder\n",
            "  Downloading htbuilder-0.6.1.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.8/dist-packages (from altair>=3.2.0->streamlit->-r requirements.txt (line 8)) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.8/dist-packages (from altair>=3.2.0->streamlit->-r requirements.txt (line 8)) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.8/dist-packages (from altair>=3.2.0->streamlit->-r requirements.txt (line 8)) (0.12.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.8/dist-packages (from breadability>=0.1.20->sumy->-r requirements.txt (line 1)) (4.0.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.8/dist-packages (from breadability>=0.1.20->sumy->-r requirements.txt (line 1)) (4.9.2)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers->-r requirements.txt (line 9)) (3.9.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers->-r requirements.txt (line 9)) (6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=1.4->streamlit->-r requirements.txt (line 8)) (3.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy->-r requirements.txt (line 4)) (2.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->-r requirements.txt (line 7)) (1.15.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.7.0->sumy->-r requirements.txt (line 1)) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.7.0->sumy->-r requirements.txt (line 1)) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.7.0->sumy->-r requirements.txt (line 1)) (2.10)\n",
            "Collecting markdown-it-py<3.0.0,>=2.2.0\n",
            "  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 KB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pygments<3.0.0,>=2.13.0\n",
            "  Downloading Pygments-2.14.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy->-r requirements.txt (line 4)) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy->-r requirements.txt (line 4)) (0.0.4)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m106.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from validators>=0.2->streamlit->-r requirements.txt (line 8)) (4.4.2)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.8/dist-packages (from htbuilder->st-annotated-text->-r requirements.txt (line 16)) (9.1.0)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit->-r requirements.txt (line 8)) (0.19.3)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit->-r requirements.txt (line 8)) (5.12.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit->-r requirements.txt (line 8)) (22.2.0)\n",
            "Collecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Building wheels for collected packages: sentence-transformers, cdifflib, summa, keybert, pke, st-annotated-text, breadability, docopt, pycountry, validators, htbuilder, jellyfish\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=2a478a519aadf20886181ff46bbdc095703f764bcacc077b26fb84ef30084a08\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/6f/8c/d88aec621f3f542d26fac0342bef5e693335d125f4e54aeffe\n",
            "  Building wheel for cdifflib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cdifflib: filename=cdifflib-1.2.6-cp38-cp38-linux_x86_64.whl size=37352 sha256=ac8d6764aa45f1d3d1920d58f1fd23989350bb7264301b8ff35948a15cd42a93\n",
            "  Stored in directory: /root/.cache/pip/wheels/0f/ae/42/928075504d5f17237e6efae65604402e73598b83d1f4c77b0f\n",
            "  Building wheel for summa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for summa: filename=summa-1.2.0-py3-none-any.whl size=54410 sha256=b74128686bdb2dd88a662384b35f777667e6ceab65f5bc1371a722068ed02d21\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/6a/dd/209eb19d5f2266b9cfd06827539bf70435b0ad5fe8244e52d3\n",
            "  Building wheel for keybert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keybert: filename=keybert-0.7.0-py3-none-any.whl size=23800 sha256=05c0bca3d01a745f58e7ba7ff1ada58dd40f03bac0857b113905d0a29e6b082b\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/bc/8b/a51bee77aec33895e6c8c236144b4cc10875659c4d2c80f070\n",
            "  Building wheel for pke (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pke: filename=pke-2.0.0-py3-none-any.whl size=6160288 sha256=fb8d950ef3f269f73045fa57ba3cc4c396c5f38ffb07d6923233ec318ee2036d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-t2_nxo9o/wheels/29/30/7c/66e4c0828efc7caa6e0369904987d3687d2ebe0ab404367fa1\n",
            "  Building wheel for st-annotated-text (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for st-annotated-text: filename=st_annotated_text-3.0.0-py3-none-any.whl size=8345 sha256=b0e03ea4203353a20a57d0d42d538c83d9d459d71ad45f504c1dd9e1bda56c54\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/a8/5d/260ae7928e4fda1b312661a7e88d6d5d59ade5360cba7dfebe\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21714 sha256=340f6426435cfd465b4633a3c6164f422e491087e8c964530217773e8b98a526\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/0d/0c/2062d8c1758b4b1a2e42b4a63e6660d9ec2ba9463cfee9eeab\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=4a18ca8f9c414783d3df3af5a094f82a5ae82b4302a4261f155cbd4774c894cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/ea/58/ead137b087d9e326852a851351d1debf4ada529b6ac0ec4e8c\n",
            "  Building wheel for pycountry (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycountry: filename=pycountry-22.3.5-py2.py3-none-any.whl size=10681845 sha256=78c39058b2ef751c3b49abd88a6014cda74d29b4a8e086cceef56641dfac46bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/e2/aa/0f/c224e473b464387170b83ca7c66947b4a7e33e8d903a679748\n",
            "  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19581 sha256=75a42a1ed2e64248d6c416694a400d54160d113c0d6d22fe27c584cce5535ca0\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/09/72/3eb74d236bb48bd0f3c6c3c83e4e0c5bbfcbcad7c6c3539db8\n",
            "  Building wheel for htbuilder (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for htbuilder: filename=htbuilder-0.6.1-py3-none-any.whl size=12458 sha256=acc46a9dcea99a6e27db60c1c6305b656a90595b4e4eca79589309a790736beb\n",
            "  Stored in directory: /root/.cache/pip/wheels/35/b2/79/38294ef56a2080e398b0647d40e3e5a9a2d0064bce33896d98\n",
            "  Building wheel for jellyfish (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jellyfish: filename=jellyfish-0.9.0-cp38-cp38-linux_x86_64.whl size=77931 sha256=c67727e6264ead586e162c3168dedfd8f92dfcfcfa159364173cbcc462ea7b84\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/c7/3c/4c83132de76359e3a429fd09c08995945ca96c5290a41651d3\n",
            "Successfully built sentence-transformers cdifflib summa keybert pke st-annotated-text breadability docopt pycountry validators htbuilder jellyfish\n",
            "Installing collected packages: tokenizers, sentencepiece, docopt, cdifflib, watchdog, validators, unidecode, smmap, semver, segtok, pysbd, pympler, pygments, pycountry, mdurl, jellyfish, htbuilder, breadability, blinker, yake, sumy, summa, st-annotated-text, pydeck, markdown-it-py, huggingface-hub, gitdb, transformers, rich, gitpython, streamlit, sentence-transformers, pke, keybert\n",
            "  Attempting uninstall: pygments\n",
            "    Found existing installation: Pygments 2.6.1\n",
            "    Uninstalling Pygments-2.6.1:\n",
            "      Successfully uninstalled Pygments-2.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed blinker-1.5 breadability-0.1.20 cdifflib-1.2.6 docopt-0.6.2 gitdb-4.0.10 gitpython-3.1.31 htbuilder-0.6.1 huggingface-hub-0.12.1 jellyfish-0.9.0 keybert-0.7.0 markdown-it-py-2.2.0 mdurl-0.1.2 pke-2.0.0 pycountry-22.3.5 pydeck-0.8.0 pygments-2.14.0 pympler-1.0.1 pysbd-0.3.4 rich-13.3.2 segtok-1.5.11 semver-2.13.0 sentence-transformers-2.2.2 sentencepiece-0.1.97 smmap-5.0.0 st-annotated-text-3.0.0 streamlit-1.19.0 summa-1.2.0 sumy-0.11.0 tokenizers-0.13.2 transformers-4.26.1 unidecode-1.3.6 validators-0.20.0 watchdog-2.3.1 yake-0.4.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26c19178",
      "metadata": {
        "id": "26c19178"
      },
      "source": [
        "# Keyword Collection Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "97a52cdb",
      "metadata": {
        "id": "97a52cdb",
        "outputId": "f175e259-e630-4f20-acb4-c0d97612f3c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "from contrastive_keyword_extraction import contrastive_extraction, final_score\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "from cleantext import clean\n",
        "from baselines import *\n",
        "from tqdm import tqdm\n",
        "import string\n",
        "import pickle\n",
        "import sentence_comparision\n",
        "import sentence_importance\n",
        "import summary\n",
        "import utilities\n",
        "import keyword_extraction\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "885a2b6d",
      "metadata": {
        "id": "885a2b6d"
      },
      "outputs": [],
      "source": [
        "with open(\"policy_data.pkl\", \"rb\") as file:\n",
        "  policies = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VaTnBJpCkdoj"
      },
      "id": "VaTnBJpCkdoj",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "f196a451",
      "metadata": {
        "id": "f196a451"
      },
      "outputs": [],
      "source": [
        "def create_collection(sites, \n",
        "                      ke_extractor = keyword_extraction.extract_yake, \n",
        "                      num_keywords=10,\n",
        "                      max_ngram=2, \n",
        "                      sentence_matcher = sentence_comparision.match_sentences_semantic_search,\n",
        "                      importance_estimator = sentence_importance.text_rank_importance,\n",
        "                      use_furthest=False, \n",
        "                      name_prefix=\"\",\n",
        "                      make_data_persistent=False, \n",
        "                      path=\"dataframes\",\n",
        "                      threshold=0.6,\n",
        "                      stopwords=[],\n",
        "                      combinator=utilities.alpha_combination,\n",
        "                      gamma = 0.5,\n",
        "                      num_splits=1,\n",
        "                      matching_model=\"all-MiniLM-L6-v2\"):\n",
        "    \n",
        "    for i in tqdm(sites.keys()):\n",
        "\n",
        "        documents = sites[i]\n",
        "        \n",
        "        # run CKE-pipeline\n",
        "        # Extract Keywords, and Matched sentences\n",
        "        keywords, matched_dict, changed_indices, additions, deletions, new_indices, ranking, removed, matched_indices, unified_delitions = contrastive_extraction(documents, \n",
        "                                                                            max_ngram=max_ngram,\n",
        "                                                                            min_ngram=1, \n",
        "                                                                            show_changes=False, \n",
        "                                                                            symbols_to_remove=string.punctuation,\n",
        "                                                                            importance_estimator=importance_estimator,\n",
        "                                                                            match_sentences=sentence_matcher,\n",
        "                                                                            threshold=threshold,\n",
        "                                                                            extra_stopwords=stopwords,\n",
        "                                                                            top_k=int(num_splits),\n",
        "                                                                            combinator=combinator,\n",
        "                                                                            alpha_gamma=gamma,\n",
        "                                                                            matching_model=matching_model)\n",
        "        \n",
        "        \n",
        "\n",
        "    \n",
        "        # create itermediate\n",
        "        kws, scores = extract_from_dict(keywords)\n",
        "        \n",
        "        pipeline_frame = pd.DataFrame({'keyword': kws, 'score': scores})\n",
        "        \n",
        "        #extractor = lambda x: ke_extractor(x, max_ngram_size=max_ngram, numOfKeywords=num_keywords)\n",
        "        \n",
        "        # create CKE on the specified baseline\n",
        "        baseline_keywords = baseline_diff_content(additions, unified_delitions, ke_extractor, num_keywords, max_ngram)\n",
        "        \n",
        "        baseline_kws, baseline_scores = extract_from_tuple_list(baseline_keywords)\n",
        "        \n",
        "        baseline_frame1 = pd.DataFrame({'keyword': baseline_kws, 'score': baseline_scores})\n",
        "        \n",
        "        \n",
        "        # create CKE for baseline method 2\n",
        "        baseline_keywords2 = baseline_keywords_in_diff(documents, ke_extractor, additions, deletions, candidates=50, max_ngram=max_ngram)\n",
        "        \n",
        "        baseline_kws2, baseline_scores2 = extract_from_dict(baseline_keywords2)\n",
        "        \n",
        "        baseline_frame2 = pd.DataFrame({'keyword': baseline_kws2, 'score': baseline_scores2})\n",
        "        \n",
        "        # Baseline 3\n",
        "        baseline_keywords3 = baseline3(documents, additions, unified_delitions, max_ngram)\n",
        "        \n",
        "        baseline_kws3, baseline_scores3 = extract_from_dict(baseline_keywords3)\n",
        "        \n",
        "        baseline_frame3 = pd.DataFrame({'keyword': baseline_kws3, 'score': baseline_scores3})\n",
        "\n",
        "        # Baseline 4\n",
        "        baseline_keywords4 = baseline4(documents, max_ngram, stopwords)\n",
        "        \n",
        "        baseline_kws4, baseline_scores4 = extract_from_dict(baseline_keywords4)\n",
        "        \n",
        "        baseline_frame4 = pd.DataFrame({'keyword': baseline_kws4, 'score': baseline_scores4})\n",
        "\n",
        "        # decide, wether to actually save the data\n",
        "        if make_data_persistent:\n",
        "            \n",
        "            pipeline_frame.to_csv(f\"{path}/{name_prefix}_pipeline_keywords_{i}.csv\", index=False)\n",
        "            \n",
        "            baseline_frame1.to_csv(f\"{path}/{name_prefix}_baseline1_keywords_{i}.csv\", index=False)\n",
        "            \n",
        "            baseline_frame2.to_csv(f\"{path}/{name_prefix}_baseline2_keywords_{i}.csv\", index=False)\n",
        "            \n",
        "            baseline_frame3.to_csv(f\"{path}/{name_prefix}_baseline3_keywords_{i}.csv\", index=False)\n",
        "            \n",
        "            baseline_frame4.to_csv(f\"{path}/{name_prefix}_baseline4_keywords_{i}.csv\", index=False)\n",
        "    \n",
        "    return pipeline_frame, baseline_frame1, baseline_frame2, baseline_frame3, baseline_frame4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "042df25e",
      "metadata": {
        "id": "042df25e",
        "outputId": "3f2d5476-ecc6-4173-eafa-578e1a9e94fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▍         | 1/25 [00:27<10:50, 27.09s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-37c0cc53259f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m create_collection(policies, \n\u001b[0m\u001b[1;32m      2\u001b[0m                   \u001b[0mke_extractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeyword_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_yake\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                   \u001b[0mnum_keywords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                   \u001b[0mmax_ngram\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                   \u001b[0msentence_matcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence_comparision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_sentences_semantic_search\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-8584fe8ae60f>\u001b[0m in \u001b[0;36mcreate_collection\u001b[0;34m(sites, ke_extractor, num_keywords, max_ngram, sentence_matcher, importance_estimator, use_furthest, name_prefix, make_data_persistent, path, threshold, stopwords, combinator, gamma, num_splits, matching_model)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# run CKE-pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Extract Keywords, and Matched sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         keywords, matched_dict, changed_indices, additions, deletions, new_indices, ranking, removed, matched_indices, unified_delitions = contrastive_extraction(documents, \n\u001b[0m\u001b[1;32m     25\u001b[0m                                                                             \u001b[0mmax_ngram\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_ngram\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                                                                             \u001b[0mmin_ngram\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/CKE-Evaluation/contrastive_keyword_extraction.py\u001b[0m in \u001b[0;36mcontrastive_extraction\u001b[0;34m(documents, max_ngram, min_ngram, importance_estimator, combinator, threshold, top_k, alpha_gamma, matching_model, w0, w1, w2, match_sentences, show_changes, symbols_to_remove, extra_stopwords)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;31m# Perform Matching\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;31m# matchers: semantic_search, weighted_tfidf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0mmatched_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremoved\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatching_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/CKE-Evaluation/sentence_comparision.py\u001b[0m in \u001b[0;36mmatch_sentences_semantic_search\u001b[0;34m(document_a, document_b, threshold, k, model)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0mseg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpysbd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSegmenter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m     \u001b[0mqueries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;31m# Use the sentences in B as our corpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pysbd/segmenter.py\u001b[0m in \u001b[0;36msegment\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleaner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mpostprocessed_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0msentence_w_char_spans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_with_char_spans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpostprocessed_sents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar_span\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pysbd/processor.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mli\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mListItemReplacer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mli\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace_abbreviations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace_numbers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace_continuous_punctuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pysbd/processor.py\u001b[0m in \u001b[0;36mreplace_abbreviations\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreplace_abbreviations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabbreviations_replacer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbetween_punctuation_processor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pysbd/abbreviation_replacer.py\u001b[0m in \u001b[0;36mreplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mabbr_handled_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mabbr_handled_text\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_for_abbreviations_in_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabbr_handled_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace_multi_period_abbreviations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pysbd/abbreviation_replacer.py\u001b[0m in \u001b[0;36msearch_for_abbreviations_in_string\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mchar_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_word_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabbrev_match\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 text = self.scan_for_replacements(\n\u001b[0m\u001b[1;32m     93\u001b[0m                     \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchar_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pysbd/abbreviation_replacer.py\u001b[0m in \u001b[0;36mscan_for_replacements\u001b[0;34m(self, txt, am, ind, char_array)\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0mtxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplace_prepositive_abbr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnumber_abbr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                 \u001b[0mtxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplace_pre_number_abbr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mtxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace_period_of_abbr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pysbd/abbreviation_replacer.py\u001b[0m in \u001b[0;36mreplace_pre_number_abbr\u001b[0;34m(txt, abbr)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# prepend a space to avoid needing another regex for start of string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtxt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"(?<=\\s{abbr})\\.(?=(\\s\\d|\\s+\\())\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabbr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mabbr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"∯\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m# remove the prepended space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtxt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/re.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "create_collection(policies, \n",
        "                  ke_extractor = keyword_extraction.extract_yake, \n",
        "                  num_keywords=15,\n",
        "                  max_ngram=2, \n",
        "                  sentence_matcher = sentence_comparision.match_sentences_semantic_search,\n",
        "                  importance_estimator = sentence_importance.text_rank_importance,\n",
        "                  use_furthest=False, \n",
        "                  name_prefix=\"standard\",\n",
        "                  make_data_persistent=True, \n",
        "                  path=\"Dataframes\",\n",
        "                  threshold=0.65,\n",
        "                  stopwords=nltk.corpus.stopwords.words(\"english\"),\n",
        "                  combinator=utilities.alpha_combination,\n",
        "                  gamma = 0.5,\n",
        "                  num_splits=1,\n",
        "                  matching_model =\"msmarco-distilbert-base-v4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3261d295",
      "metadata": {
        "id": "3261d295"
      },
      "outputs": [],
      "source": [
        "from itertools import product\n",
        "\n",
        "\n",
        "def cartesian_product(params):\n",
        "    \n",
        "    # gett all possible combinations\n",
        "    return list(product(*params.values()))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a582c3af",
      "metadata": {
        "id": "a582c3af"
      },
      "source": [
        "# Parameters to Use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b825a0e",
      "metadata": {
        "id": "6b825a0e"
      },
      "outputs": [],
      "source": [
        "parameters = {\"matcher\": [sentence_comparision.match_sentences_semantic_search,\n",
        "                         sentence_comparision.match_sentences_tfidf_weighted],\n",
        "              \n",
        "             \"ie\": [sentence_importance.text_rank_importance,\n",
        "                    sentence_importance.yake_weighted_importance],\n",
        "              \n",
        "             \"threshold\": [0.5, 0.6, 0.7]\n",
        "              \n",
        "             }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9ef939c",
      "metadata": {
        "id": "b9ef939c",
        "outputId": "3031d2f6-11e3-40c1-836d-26ed69fc6f30"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(cartesian_product(parameters))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0477778",
      "metadata": {
        "id": "c0477778"
      },
      "outputs": [],
      "source": [
        "def create_cartesian_collection(params, \n",
        "                                sites, \n",
        "                                baseline_ke_extractor = keyword_extraction.extract_yake, \n",
        "                                num_keywords=10, \n",
        "                                max_ngram=2, \n",
        "                                use_furthest=False,\n",
        "                                make_data_persistent=False,\n",
        "                                file_prefix = \"combination\",\n",
        "                                path=\"dataframes\",\n",
        "                                compare_k = 15):\n",
        "            \n",
        "        \n",
        "    combinations = cartesian_product(params)\n",
        "    \n",
        "    number_of_combinations = len(combinations)\n",
        "    \n",
        "    count = 0\n",
        "    \n",
        "    for combination in combinations:\n",
        "        \n",
        "        matcher, threshold = combination\n",
        "        \n",
        "        print(f\"Contrastive Keyword Extraction pipeline is being ran with combination {count}:\")\n",
        "        \n",
        "        pipeline_frame, baseline_frame1, baseline_frame2, baseline_frame3, baseline_frame4 = create_collection( sites = sites, \n",
        "                                                                  ke_extractor = baseline_ke_extractor, \n",
        "                                                                  num_keywords = num_keywords, \n",
        "                                                                  max_ngram = max_ngram, \n",
        "                                                                  sentence_matcher = matcher,\n",
        "                                                                  use_furthest = use_furthest,\n",
        "                                                                  name_prefix=f\"{file_prefix}_{count}\",\n",
        "                                                                  make_data_persistent=make_data_persistent,\n",
        "                                                                  path=path,\n",
        "                                                                  threshold=threshold)\n",
        "        \n",
        "      \n",
        "        \n",
        "        count += 1\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "316c4144",
      "metadata": {
        "id": "316c4144"
      },
      "outputs": [],
      "source": [
        "create_cartesian_collection(parameters,\n",
        "                            policies\n",
        "                            baseline_ke_extractor = keyword_extraction.extract_yake,\n",
        "                            num_keywords=20,\n",
        "                            max_ngram=2,\n",
        "                            use_furthest=True, # only compare the first and last document\n",
        "                            make_data_persistent=True,\n",
        "                            file_prefix=\"threshold\",\n",
        "                            path=\"Combinations\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}